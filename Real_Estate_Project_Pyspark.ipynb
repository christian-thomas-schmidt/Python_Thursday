{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Real Estate Project Pyspark.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMjE8rKptH+st4qrfiAgMwl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christian-thomas-schmidt/Python_Thursday/blob/main/Real_Estate_Project_Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Christian Schmidt: Real Estate Dataset\n",
        "\n",
        "This is my MET CS 777 project using pyspark and machine learning in Python.<br>\n",
        "The dataset is provided by Ahmed Shahriar Sakib on Kaggle.com.<br><br>\n",
        "The <b>\"Goal\"</b> of this project is to see whether I can predict house prices based on the features provided using machine learning in pyspark.<br><br>\n",
        "I expect to use Linear Regression algorithm to create a model and I hypothesize that the greater the features (more beds/baths, a bigger acre_lot, larger house_size) the more expensive a house will be.<br><br>\n",
        "The link for the dataset can be found [here](https://www.kaggle.com/datasets/ahmedshahriarsakib/usa-real-estate-dataset?resource=download).<br>\n",
        "\n",
        "### Context:\n",
        "This dataset contains Real Estate listings in the US broken by State and zip code.<br> \n",
        "Data was collected via web scraping using python libraries.\n",
        "\n",
        "### Content:\n",
        "\n",
        "--- The dataset has 1 CSV file with 12 columns ---\n",
        "\n",
        "*   realtor-data.csv (200k+ entries)\n",
        "*   status\n",
        "*   bed\n",
        "*   bath\n",
        "*   acre_lot\n",
        "*   full_address\n",
        "*   street\n",
        "*   city\n",
        "*   state\n",
        "*   zip_code\n",
        "*   house_size\n",
        "*   sold_date\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "19S_3xvhui4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First step is to download the data and import into google colab enviroment"
      ],
      "metadata": {
        "id": "yiNmEbHav99M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVxCNoKBreqX",
        "outputId": "2cde1421-c01d-4e8d-ad1a-83f13d472f15"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.6.15)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "9bHBdt3Arfqc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "_SkOmALkrfkO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "PMBxrXVMrfdj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3dxN-dRoqQM",
        "outputId": "a583266e-acbc-4fe6-d31f-ddd17faf199c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading usa-real-estate-dataset.zip to /content\n",
            "\r  0% 0.00/5.07M [00:00<?, ?B/s]\n",
            "\r100% 5.07M/5.07M [00:00<00:00, 77.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d ahmedshahriarsakib/usa-real-estate-dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip usa-real-estate-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4LCKfsntz4c",
        "outputId": "5829fdb6-b411-4349-a95d-5cc87a69e5ab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  usa-real-estate-dataset.zip\n",
            "  inflating: realtor-data.csv        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries"
      ],
      "metadata": {
        "id": "oZjQJwoswKF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --ignore-installed -q pyspark==3.2.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VU3dvX87H2mt",
        "outputId": "4152f5c4-ed69-4aed-dbae-07b13b4f97da"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.4 MB 37 kB/s \n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/py4j/\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 198 kB 49.6 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.feature import VectorAssembler"
      ],
      "metadata": {
        "id": "4XP18tmJwHgm"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\""
      ],
      "metadata": {
        "id": "qRaOF0m-glAn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext.getOrCreate()\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "3Ndk5LVGJ4S2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the Dataset\n",
        "I want to gather information about this dataset before I begin my clean-up process."
      ],
      "metadata": {
        "id": "ZdHlxkFGwRi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.option(\"header\",True).csv('realtor-data.csv')\n",
        "df.printSchema()\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfxzedfQwDTP",
        "outputId": "4e3379d7-9798-4a3c-b187-ab942666f3c7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- status: string (nullable = true)\n",
            " |-- price: string (nullable = true)\n",
            " |-- bed: string (nullable = true)\n",
            " |-- bath: string (nullable = true)\n",
            " |-- acre_lot: string (nullable = true)\n",
            " |-- full_address: string (nullable = true)\n",
            " |-- street: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- zip_code: string (nullable = true)\n",
            " |-- house_size: string (nullable = true)\n",
            " |-- sold_date: string (nullable = true)\n",
            "\n",
            "+--------+--------+---+----+--------+--------------------+--------------------+----------+-----------+--------+----------+---------+\n",
            "|  status|   price|bed|bath|acre_lot|        full_address|              street|      city|      state|zip_code|house_size|sold_date|\n",
            "+--------+--------+---+----+--------+--------------------+--------------------+----------+-----------+--------+----------+---------+\n",
            "|for_sale|105000.0|3.0| 2.0|    0.12|Sector Yahuecas T...|Sector Yahuecas T...|  Adjuntas|Puerto Rico|   601.0|     920.0|     null|\n",
            "|for_sale| 80000.0|4.0| 2.0|    0.08|Km 78 9 Carr # 13...|  Km 78 9 Carr # 135|  Adjuntas|Puerto Rico|   601.0|    1527.0|     null|\n",
            "|for_sale| 67000.0|2.0| 1.0|    0.15|556G 556-G 16 St,...|    556G 556-G 16 St|Juana Diaz|Puerto Rico|   795.0|     748.0|     null|\n",
            "|for_sale|145000.0|4.0| 2.0|     0.1|R5 Comunidad El P...|R5 Comunidad El P...|     Ponce|Puerto Rico|   731.0|    1800.0|     null|\n",
            "|for_sale| 65000.0|6.0| 2.0|    0.05|14 Navarro, Mayag...|          14 Navarro|  Mayaguez|Puerto Rico|   680.0|      null|     null|\n",
            "+--------+--------+---+----+--------+--------------------+--------------------+----------+-----------+--------+----------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to work with FloatType() for all numbers for this analyses, let's change the schema to incorporate this"
      ],
      "metadata": {
        "id": "jysv3DWlhS2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,FloatType\n",
        "\n",
        "schema = StructType([StructField(\"status\",StringType(),True),\n",
        "                     StructField(\"price\",FloatType(),True),\n",
        "                     StructField(\"bed\",FloatType(),True),\n",
        "                     StructField(\"bath\",FloatType(),True),\n",
        "                     StructField(\"acre_lot\",FloatType(),True),\n",
        "                     StructField(\"full_address\",StringType(),True),\n",
        "                     StructField(\"street\",StringType(),True),\n",
        "                     StructField(\"city\",StringType(),True),\n",
        "                     StructField(\"state\",StringType(),True),\n",
        "                     StructField(\"zip_code\",StringType(),True),\n",
        "                     StructField(\"house_size\",FloatType(),True),\n",
        "                     StructField(\"sold_date\",StringType(),True)])\n"
      ],
      "metadata": {
        "id": "NhUiDdNWhZh6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv('realtor-data.csv', header=True, schema=schema)\n",
        "df.printSchema()\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mH3zHynxxy1O",
        "outputId": "b7bced5f-2528-4549-9e9b-d94211e02c4c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- status: string (nullable = true)\n",
            " |-- price: float (nullable = true)\n",
            " |-- bed: float (nullable = true)\n",
            " |-- bath: float (nullable = true)\n",
            " |-- acre_lot: float (nullable = true)\n",
            " |-- full_address: string (nullable = true)\n",
            " |-- street: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- zip_code: string (nullable = true)\n",
            " |-- house_size: float (nullable = true)\n",
            " |-- sold_date: string (nullable = true)\n",
            "\n",
            "+--------+--------+---+----+--------+--------------------+--------------------+----------+-----------+--------+----------+---------+\n",
            "|  status|   price|bed|bath|acre_lot|        full_address|              street|      city|      state|zip_code|house_size|sold_date|\n",
            "+--------+--------+---+----+--------+--------------------+--------------------+----------+-----------+--------+----------+---------+\n",
            "|for_sale|105000.0|3.0| 2.0|    0.12|Sector Yahuecas T...|Sector Yahuecas T...|  Adjuntas|Puerto Rico|   601.0|     920.0|     null|\n",
            "|for_sale| 80000.0|4.0| 2.0|    0.08|Km 78 9 Carr # 13...|  Km 78 9 Carr # 135|  Adjuntas|Puerto Rico|   601.0|    1527.0|     null|\n",
            "|for_sale| 67000.0|2.0| 1.0|    0.15|556G 556-G 16 St,...|    556G 556-G 16 St|Juana Diaz|Puerto Rico|   795.0|     748.0|     null|\n",
            "|for_sale|145000.0|4.0| 2.0|     0.1|R5 Comunidad El P...|R5 Comunidad El P...|     Ponce|Puerto Rico|   731.0|    1800.0|     null|\n",
            "|for_sale| 65000.0|6.0| 2.0|    0.05|14 Navarro, Mayag...|          14 Navarro|  Mayaguez|Puerto Rico|   680.0|      null|     null|\n",
            "+--------+--------+---+----+--------+--------------------+--------------------+----------+-----------+--------+----------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print((df.count(), len(df.columns)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3WNjLmUx2EJ",
        "outputId": "07d8bcde-def8-4118-ca38-e2db615d72d8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(203126, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With only 203,126 rows of data, I wonder what states this data includes?"
      ],
      "metadata": {
        "id": "XAAaGLgqx-bV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_array = np.array(df.select(\"state\").collect())\n",
        "state_unique, state_count = np.unique(state_array, return_counts=True)\n",
        "result = np.column_stack((state_unique, state_count))\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXgNGqY7lr5H",
        "outputId": "7342a828-96d1-4581-9715-41d0e3dd57fb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Connecticut' '12207']\n",
            " ['Massachusetts' '150792']\n",
            " ['New Hampshire' '4721']\n",
            " ['New Jersey' '2']\n",
            " ['New York' '1874']\n",
            " ['Puerto Rico' '24679']\n",
            " ['Rhode Island' '4907']\n",
            " ['South Carolina' '24']\n",
            " ['Tennessee' '18']\n",
            " ['Vermont' '1324']\n",
            " ['Virgin Islands' '2573']\n",
            " ['Virginia' '5']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "with 150,000 homes, the dataset is mostly focused on the housing market in Massachusetts.<br> With this information I may want to narrow the scope of my project to just look at the housing market in Massachusetts.<br><br> However, what if there are duplicates?"
      ],
      "metadata": {
        "id": "A62s6ONMz8rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_duplicates = df.groupBy(\"full_address\").count().filter(\"count > 1\")\n",
        "df_duplicates.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zjVMjwym2MF",
        "outputId": "4d291d1e-3a9f-4bb7-bd31-e344bfaa5be0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|        full_address|count|\n",
            "+--------------------+-----+\n",
            "|17 Estancias Del ...|   14|\n",
            "|499 Calle Reparto...|    7|\n",
            "|Villa Taina Guani...|    6|\n",
            "|1 Calle Independe...|    6|\n",
            "|G9 Serenidad, Coa...|   15|\n",
            "|Arecibo Bo Hato A...|    8|\n",
            "|1 Calle Cervantes...|   17|\n",
            "|44 Grange Stock E...|    4|\n",
            "|2-10 Recon Concor...|    2|\n",
            "|29 Pleasant St Ap...|   27|\n",
            "|Old Stage Rd Lot ...|   33|\n",
            "|24 Worcester Rd, ...|    7|\n",
            "|37 Marian Ave, Pi...|   16|\n",
            "|4 Cooper Rd, Grea...|   12|\n",
            "|37 Orchard St, Ad...|   19|\n",
            "|181 Holmes Rd, Pi...|   15|\n",
            "|71 Tracy Cir, Win...|    9|\n",
            "|28-30 W Main St, ...|   22|\n",
            "|19 Stone Xing Lot...|   14|\n",
            "|221 Route 87, Col...|    3|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_duplicates.select(\"count\").groupBy().sum().collect()[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceJZXhVln5oY",
        "outputId": "4b4e5ff2-f8e3-4bf4-baec-439d5060e3a7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "198133"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vast majority of this dataset includes duplicate rows, I think it would be best to remove these and then see what the distrubtion is at the state level."
      ],
      "metadata": {
        "id": "Jv6RwHOp0t_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.drop_duplicates()\n",
        "#df2 = df.distinct"
      ],
      "metadata": {
        "id": "8_XDqsnp0le8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# repeat prior step, but include percent distribution\n",
        "def state_table(df):\n",
        "  state_array = np.array(df.select(\"state\").collect())\n",
        "  state_unique, state_count = np.unique(state_array, return_counts=True)\n",
        "  state_percent = [\"%.3f%%\" % elem for elem in list(state_count*100/len(state_array))]\n",
        "  state_df = pd.DataFrame({'State':state_unique,'Count':state_count,\"% Dist.\":state_percent}).sort_values(by=['Count'],ascending=False)\n",
        "  return(state_df)"
      ],
      "metadata": {
        "id": "T7t0_Qos03fd"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(state_table(df2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgldjvQMu-g0",
        "outputId": "daf4590f-ee67-4588-b377-64c5c8933f6f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             State  Count  % Dist.\n",
            "1    Massachusetts   9514  45.480%\n",
            "0      Connecticut   3870  18.500%\n",
            "5      Puerto Rico   2664  12.735%\n",
            "6     Rhode Island   2117  10.120%\n",
            "2    New Hampshire    965   4.613%\n",
            "4         New York    800   3.824%\n",
            "10  Virgin Islands    750   3.585%\n",
            "9          Vermont    235   1.123%\n",
            "3       New Jersey      1   0.005%\n",
            "7   South Carolina      1   0.005%\n",
            "8        Tennessee      1   0.005%\n",
            "11        Virginia      1   0.005%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given this insight, I will shift my focus to building my model to predict Massachusetts housing prices.<br>\n",
        "I will then use Conneticut as a control group to analyze the affectiveness cross border.<br><br>\n",
        "Finally I will look at the housing market in Puerto Rico and see if I can find any unique differences and see if the model is also applicable to the territory."
      ],
      "metadata": {
        "id": "m3i3MrcX73cn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-processing\n",
        "I will limit the scope of this project to the state level.<br>\n",
        "With that I will pre-process data to prepare the dataset for modeling."
      ],
      "metadata": {
        "id": "iciWV32F8clJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# since we want to predict home prices, I will remove unwanted columns.\n",
        "df3 = df2.drop('status','full_address','street','zip_code','sold_date')\n",
        "df3.printSchema()"
      ],
      "metadata": {
        "id": "Oq3x8yfi3B67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cf95b37-2f8d-41d4-f08e-d0e5065e818c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- price: float (nullable = true)\n",
            " |-- bed: float (nullable = true)\n",
            " |-- bath: float (nullable = true)\n",
            " |-- acre_lot: float (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- house_size: float (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next step, I will remove any na values as I want avoid  bias in my data and only deal with known features.<br>"
      ],
      "metadata": {
        "id": "iiBtY3HivLGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we want to remove any rows that have null values\n",
        "df3 = df3.na.drop(\"any\")"
      ],
      "metadata": {
        "id": "cb-eEXNBtIv3"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's obserbe that data after the removal of NA values \n",
        "print((df3.count(), len(df3.columns)))\n",
        "print(state_table(df3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KURHC1_9ucvi",
        "outputId": "7c78cb13-080b-4a6f-e239-8cfd68be6b73"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13069, 7)\n",
            "            State  Count  % Dist.\n",
            "1   Massachusetts   5772  44.166%\n",
            "0     Connecticut   2805  21.463%\n",
            "6    Rhode Island   1788  13.681%\n",
            "5     Puerto Rico   1548  11.845%\n",
            "2   New Hampshire    501   3.833%\n",
            "4        New York    423   3.237%\n",
            "7         Vermont    126   0.964%\n",
            "8  Virgin Islands    105   0.803%\n",
            "3      New Jersey      1   0.008%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm happy with the distribution and glad to see that Massachusetts still remains as the largest count of homes. From here I will create subset the dataset with the 3 states I am interested in.\n",
        "\n",
        "1.   Massachusetts\n",
        "2.   Conneticut\n",
        "3.   Puerto Rico"
      ],
      "metadata": {
        "id": "kvrQXLAJvhKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_3_states = df3.where((df3.state == 'Massachusetts')\\\n",
        "                        | (df3.state == 'Connecticut')\\\n",
        "                        | (df3.state == 'Puerto Rico'))\n",
        "\n",
        "print(state_table(df_3_states))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HNrTfmOvhbs",
        "outputId": "bdcba5dc-dbf5-41fa-9d2e-2b23f8333ef0"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           State  Count  % Dist.\n",
            "1  Massachusetts   5772  57.007%\n",
            "0    Connecticut   2805  27.704%\n",
            "2    Puerto Rico   1548  15.289%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post-Exploratory Analysis\n",
        "\n",
        "I performed Exploratory Analysis in Python before I conducted the same process in Pyspark. These are the results from the process."
      ],
      "metadata": {
        "id": "pNRpNEPj6rYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_3_states = df_3_states.where((df_3_states.house_size < 100000)\\\n",
        "                         & (df_3_states.acre_lot < 9000)\\\n",
        "                         & (df_3_states.bath < 100)\\\n",
        "                         & (df_3_states.price < 40000000))"
      ],
      "metadata": {
        "id": "0Rfy6Zit6Bd0"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((df_3_states.count(), df3.count()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM23hoyb9Hvr",
        "outputId": "6a38f484-dc8e-4bc9-b922-e3585514ca33"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10120, 13069)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into the 3 states and observe the summary() results for each\n",
        "\n",
        "df_massachusetts = df_3_states.where(df3.state == 'Massachusetts').drop('state','city')\n",
        "df_connecticut = df_3_states.where(df3.state == 'Connecticut').drop('state','city')\n",
        "df_puerto_rico = df_3_states.where(df3.state == 'Puerto Rico').drop('state','city')"
      ],
      "metadata": {
        "id": "XFpdsF7P-7t3"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standardize the data_set\n",
        "\n",
        "Next step is to standardize the data sets for regression modeling"
      ],
      "metadata": {
        "id": "cMoBKDM1_7KK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_massachusetts.show(5)\n",
        "df_massachusetts.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSmA8UOG_6oL",
        "outputId": "9ff5edf0-9114-4567-92bd-daf57f929f55"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---+----+--------+----------+\n",
            "|   price|bed|bath|acre_lot|house_size|\n",
            "+--------+---+----+--------+----------+\n",
            "|299900.0|3.0| 2.0|    0.22|    2037.0|\n",
            "|215000.0|2.0| 1.0|    0.22|     816.0|\n",
            "|159900.0|2.0| 2.0|    0.28|    1206.0|\n",
            "|445000.0|8.0| 6.0|    0.13|    8369.0|\n",
            "|784900.0|6.0| 2.0|    0.16|    2220.0|\n",
            "+--------+---+----+--------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- price: float (nullable = true)\n",
            " |-- bed: float (nullable = true)\n",
            " |-- bath: float (nullable = true)\n",
            " |-- acre_lot: float (nullable = true)\n",
            " |-- house_size: float (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create function to scale features into one column for regression models\n",
        "def scaled_features(df):\n",
        "  assembler = VectorAssembler(inputCols=[\"bed\", \"bath\", \"acre_lot\", \"house_size\"], \n",
        "  outputCol=\"features\")\n",
        "  output = assembler.transform(df)\n",
        "\n",
        "  scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "  scaledData = scaler.fit(output).transform(output)\n",
        "  scaledData = scaledData.select('price','scaledFeatures')\n",
        "  return scaledData"
      ],
      "metadata": {
        "id": "b_ey-9S3Eya5"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_massachusetts_scaled = scaled_features(df_massachusetts)\n",
        "df_connecticut_scaled = scaled_features(df_connecticut)\n",
        "df_puerto_rico_scaled = scaled_features(df_puerto_rico)"
      ],
      "metadata": {
        "id": "2PnCVKZzHTkC"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_massachusetts_scaled.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6msZPxIgJC0M",
        "outputId": "51f35a47-4caf-4a15-cbd6-5a497dfe1a30"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+\n",
            "|   price|      scaledFeatures|\n",
            "+--------+--------------------+\n",
            "|299900.0|[0.02352941176470...|\n",
            "|215000.0|[0.01176470588235...|\n",
            "|159900.0|[0.01176470588235...|\n",
            "|445000.0|[0.08235294117647...|\n",
            "|784900.0|[0.05882352941176...|\n",
            "+--------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training and Testing\n",
        "\n",
        "The Data is all set, I will now experiment with different models to find the the highest R2 score."
      ],
      "metadata": {
        "id": "yS9EEv6rJsqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data set into two sets \n",
        "train_data,test_data=df_massachusetts_scaled.randomSplit([0.7,0.3])\n",
        "train_data.describe().show()\n",
        "train_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rLeFTZuIV8i",
        "outputId": "8a6daf13-0aa8-491e-a7c6-6d5629519abd"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+\n",
            "|summary|             price|\n",
            "+-------+------------------+\n",
            "|  count|              4016|\n",
            "|   mean|1117820.3321713146|\n",
            "| stddev| 1753014.909359984|\n",
            "|    min|           10000.0|\n",
            "|    max|             3.0E7|\n",
            "+-------+------------------+\n",
            "\n",
            "+-------+--------------------+\n",
            "|  price|      scaledFeatures|\n",
            "+-------+--------------------+\n",
            "|10000.0|[0.02352941176470...|\n",
            "|10000.0|[0.03529411764705...|\n",
            "|24900.0|[0.07058823529411...|\n",
            "|29974.0|[0.02352941176470...|\n",
            "|39900.0|[0.0,0.0,3.005645...|\n",
            "|43900.0|[0.05882352941176...|\n",
            "|49999.0|[0.03529411764705...|\n",
            "|50000.0|[0.01176470588235...|\n",
            "|59900.0|[0.01176470588235...|\n",
            "|59900.0|[0.01176470588235...|\n",
            "|59900.0|[0.02352941176470...|\n",
            "|61500.0|[0.02352941176470...|\n",
            "|65000.0|[0.0,0.0,1.742403...|\n",
            "|75000.0|[0.01176470588235...|\n",
            "|80000.0|[0.01176470588235...|\n",
            "|84900.0|[0.01176470588235...|\n",
            "|84900.0|[0.01176470588235...|\n",
            "|84900.0|[0.01176470588235...|\n",
            "|89000.0|[0.01176470588235...|\n",
            "|99000.0|[0.01176470588235...|\n",
            "+-------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "from pyspark.ml.feature import VectorIndexer\n",
        "from pyspark.ml.evaluation import RegressionEvaluator"
      ],
      "metadata": {
        "id": "anzNLEUWKECS"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's examine 5 different regression models to see the ideal model."
      ],
      "metadata": {
        "id": "JzRPo69X-2oT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def modelsummary(model):\n",
        "    import numpy as np\n",
        "    Summary=model.summary\n",
        "    print (\"Mean squared error: % .6f\" \\\n",
        "           % Summary.meanSquaredError, \", RMSE: % .6f\" \\\n",
        "           % Summary.rootMeanSquaredError )\n",
        "    print (\"##\",\"Multiple R-squared: %f\" % Summary.r2, \", \\\n",
        "            Total iterations: %i\"% Summary.totalIterations)"
      ],
      "metadata": {
        "id": "tm5E0gvnQLx1"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def modelsummary(model):\n",
        "    Summary=model.summary\n",
        "    print(f\"R2: {Summary.r2:.6f}\")\n",
        "    print(f\"Total iterations: {Summary.totalIterations}\")\n",
        "            "
      ],
      "metadata": {
        "id": "UXvPfyZPRI28"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Regression\n",
        "\n",
        "lr = LinearRegression(featuresCol='scaledFeatures',labelCol='price')\n",
        "lr_model = lr.fit(train_data)\n",
        "lr_data = lr_model.transform(test_data)"
      ],
      "metadata": {
        "id": "s1apdLZ-Lswh"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree Regressor\n",
        "\n",
        "dt = DecisionTreeRegressor(featuresCol='scaledFeatures',labelCol='price')\n",
        "dt_model = dt.fit(train_data)\n",
        "dt_data = dt_model.transform(test_data)"
      ],
      "metadata": {
        "id": "st12n1gaZILl"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random forest regression\n",
        "\n",
        "rf = RandomForestRegressor(featuresCol='scaledFeatures',labelCol='price')\n",
        "rf_model = rf.fit(train_data)\n",
        "rf_data = rf_model.transform(test_data)"
      ],
      "metadata": {
        "id": "g7hMqYNtaWlU"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradeint Boosted Tree Regression\n",
        "\n",
        "gbtr = GBTRegressor(featuresCol='scaledFeatures', labelCol='price', maxIter=10)\n",
        "gbtr_model = gbtr.fit(train_data)\n",
        "gbtr_data = gbtr_model.transform(test_data)"
      ],
      "metadata": {
        "id": "dNmM_VSkTH1H"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluate(mdata):\n",
        " \n",
        "  rmse=RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "  rmse=rmse.evaluate(mdata) \n",
        "  \n",
        "  mae=RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"mae\")\n",
        "  mae=mae.evaluate(mdata) \n",
        "  \n",
        "  r2=RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "  r2=r2.evaluate(mdata)\n",
        "\n",
        "  print(\"RMSE: \", rmse)\n",
        "  print(\"MAE: \", mae)\n",
        "  print(\"R-squared: \", r2)"
      ],
      "metadata": {
        "id": "8rgK5XQiWxRd"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Linear Regression\")\n",
        "model_evaluate(lr_data)\n",
        "print(\"\\n\")\n",
        "print(\"Decision Tree Regressor\")\n",
        "model_evaluate(dt_data)\n",
        "print(\"\\n\")\n",
        "print(\"Random forest regression\")\n",
        "model_evaluate(rf_data)\n",
        "print(\"\\n\")\n",
        "print(\"Gradeint Boosted Tree Regression\")\n",
        "model_evaluate(gbtr_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhUIhNEtYO_G",
        "outputId": "d94703cb-959d-4fac-820f-f72e6c23de3d"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression\n",
            "RMSE:  1422559.5557176957\n",
            "MAE:  633149.5148706498\n",
            "R-squared:  0.3759482252992099\n",
            "\n",
            "\n",
            "Decision Tree Regressor\n",
            "RMSE:  1394508.929893673\n",
            "MAE:  568100.1715472434\n",
            "R-squared:  0.4003162130856852\n",
            "\n",
            "\n",
            "Random forest regression\n",
            "RMSE:  1334578.9640758822\n",
            "MAE:  557134.2087125469\n",
            "R-squared:  0.45075228566076864\n",
            "\n",
            "\n",
            "Gradeint Boosted Tree Regression\n",
            "RMSE:  1398845.948770821\n",
            "MAE:  562715.733978738\n",
            "R-squared:  0.39658029681580864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even after evaluation multiple models, Random Forest Regression model only hit a 45% for dependent variability. Let's see how it performs on the Connecticut and Puerto Rico datasets"
      ],
      "metadata": {
        "id": "7__6Lqlsaqpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_connecticut = rf_model.transform(df_connecticut_scaled)\n",
        "rf_puerto_rico = rf_model.transform(df_puerto_rico_scaled)"
      ],
      "metadata": {
        "id": "Q9z0iKhEa9kS"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Massachusetts\")\n",
        "model_evaluate(rf_data)\n",
        "print(\"\\n\")\n",
        "print(\"Connecticut\")\n",
        "model_evaluate(rf_connecticut)\n",
        "print(\"\\n\")\n",
        "print(\"Puerto Rico\")\n",
        "model_evaluate(rf_puerto_rico)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Oqykliza4S9",
        "outputId": "9a87cc17-6b5b-4c3f-9643-10acacfcb5c9"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Massachusetts\n",
            "RMSE:  1334578.9640758822\n",
            "MAE:  557134.2087125469\n",
            "R-squared:  0.45075228566076864\n",
            "\n",
            "\n",
            "Connecticut\n",
            "RMSE:  3444421.97542909\n",
            "MAE:  2513596.4233944006\n",
            "R-squared:  -29.854296694293037\n",
            "\n",
            "\n",
            "Puerto Rico\n",
            "RMSE:  1667782.8378187274\n",
            "MAE:  1116489.4911979944\n",
            "R-squared:  0.057408859916077914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model gets worse when trying to predict housing prices in Connecticut and Purto Rico.\n",
        "\n",
        "I have two conclusions as a result.\n",
        "\n",
        "I believe that the dataset is to narrow and that there may be other features that would affect a house price more than just the size, acre, bed amount, and bath amount. I could be as simple as median income, or something more complex like shortages vs. surplus of house available.\n",
        "\n",
        "It doesn't seem like one can apply a model trained on one state to another state/U.S. territory. There are far to many factors that are affecting the price that what was available in this dataset."
      ],
      "metadata": {
        "id": "cIrWIPAfbWU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lEjmQlvxbbDD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}